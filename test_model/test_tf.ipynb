{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install exifread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"tensorflow<2.11\" \"numpy<2\" tensorflow-gpu==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GPU가 할당되었습니다: GPU - /physical_device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 사용 가능한 GPU 확인\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # 0번 GPU만 할당하고 이름 출력\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    print(f\"NVIDIA GPU가 할당되었습니다: {gpus[0].device_type} - {gpus[0].name}\")\n",
    "else:\n",
    "    print(\"사용 가능한 GPU가 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6002 images belonging to 2 classes.\n",
      "Found 39428 images belonging to 2 classes.\n",
      "Found 10905 images belonging to 2 classes.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 1280)              3413024   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 1281      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,414,305\n",
      "Trainable params: 1,281\n",
      "Non-trainable params: 3,413,024\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "376/376 [==============================] - 41s 106ms/step - loss: 0.6863 - accuracy: 0.6038 - val_loss: 0.6299 - val_accuracy: 0.6524\n",
      "Epoch 2/10\n",
      "376/376 [==============================] - 39s 105ms/step - loss: 0.5430 - accuracy: 0.7294 - val_loss: 0.5826 - val_accuracy: 0.6898\n",
      "Epoch 3/10\n",
      "376/376 [==============================] - 41s 108ms/step - loss: 0.4721 - accuracy: 0.7817 - val_loss: 0.5620 - val_accuracy: 0.7074\n",
      "Epoch 4/10\n",
      "376/376 [==============================] - 40s 106ms/step - loss: 0.4390 - accuracy: 0.7992 - val_loss: 0.5534 - val_accuracy: 0.7138\n",
      "Epoch 5/10\n",
      "376/376 [==============================] - 40s 106ms/step - loss: 0.4225 - accuracy: 0.8091 - val_loss: 0.5534 - val_accuracy: 0.7160\n",
      "Epoch 6/10\n",
      "376/376 [==============================] - 40s 105ms/step - loss: 0.3969 - accuracy: 0.8262 - val_loss: 0.5445 - val_accuracy: 0.7244\n",
      "Epoch 7/10\n",
      "376/376 [==============================] - 41s 108ms/step - loss: 0.3840 - accuracy: 0.8361 - val_loss: 0.5466 - val_accuracy: 0.7237\n",
      "Epoch 8/10\n",
      "376/376 [==============================] - 40s 108ms/step - loss: 0.3859 - accuracy: 0.8292 - val_loss: 0.5332 - val_accuracy: 0.7338\n",
      "Epoch 9/10\n",
      "376/376 [==============================] - 40s 106ms/step - loss: 0.3701 - accuracy: 0.8431 - val_loss: 0.5356 - val_accuracy: 0.7326\n",
      "Epoch 10/10\n",
      "376/376 [==============================] - 40s 106ms/step - loss: 0.3665 - accuracy: 0.8459 - val_loss: 0.5356 - val_accuracy: 0.7326\n",
      "---------- Validation Evaluation ----------\n",
      "2465/2465 [==============================] - 27s 11ms/step - loss: 0.5356 - accuracy: 0.7326\n",
      "Validation Loss: 0.5356, Validation Accuracy: 0.7326\n",
      "---------- Test Evaluation ----------\n",
      "682/682 [==============================] - 7s 10ms/step - loss: 0.6157 - accuracy: 0.6880\n",
      "Test Loss: 0.6157, Test Accuracy: 0.6880\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 1) 경로 설정\n",
    "#----------------------------------------------------------------------\n",
    "base_dir = os.getcwd()  # 현재 작업 디렉토리\n",
    "train_dir = os.path.join(base_dir, 'Train')\n",
    "val_dir   = os.path.join(base_dir, 'Validation')\n",
    "test_dir  = os.path.join(base_dir, 'Test')\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 2) 학습 파라미터 설정\n",
    "#----------------------------------------------------------------------\n",
    "IMAGE_SIZE = (128, 128)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10  # 필요에 따라 조정\n",
    "NUM_CLASSES = 1  # Fake vs Real (이진분류)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 3) ImageDataGenerator 설정\n",
    "#    (데이터 증강 및 전처리)\n",
    "#----------------------------------------------------------------------\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 4) Directory로부터 이미지 로드 (flow_from_directory)\n",
    "#----------------------------------------------------------------------\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=train_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',     # Fake / Real 이진 분류\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    directory=val_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=test_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 5) 사전 학습된 EfficientNet-Lite 모델 불러오기 (TensorFlow Hub 예시)\n",
    "#    - include_top=False 또는 feature-vector 형태 모델을 사용해 상위 레이어 재구성\n",
    "#    - EfficientNet Lite0 ~ Lite4까지 다양한 버전이 있으므로, 아래 URL은 예시입니다.\n",
    "#----------------------------------------------------------------------\n",
    "efficientnet_lite_url = \"https://tfhub.dev/tensorflow/efficientnet/lite0/feature-vector/2\"\n",
    "feature_extractor = hub.KerasLayer(\n",
    "    efficientnet_lite_url,\n",
    "    input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n",
    "    trainable=False  # 혹은 True로 설정하면 파라미터도 업데이트함(Full Fine-tuning)\n",
    ")\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 6) 모델 구성\n",
    "#----------------------------------------------------------------------\n",
    "model = models.Sequential([\n",
    "    feature_extractor,\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation='sigmoid')  # 이진 분류이므로 sigmoid 사용\n",
    "])\n",
    "\n",
    "model.summary()  # 모델 구조 확인\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 7) 모델 컴파일\n",
    "#----------------------------------------------------------------------\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 8) 모델 학습 (Fine-tuning)\n",
    "#----------------------------------------------------------------------\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator\n",
    ")\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 9) 학습 결과 평가\n",
    "#----------------------------------------------------------------------\n",
    "print(\"---------- Validation Evaluation ----------\")\n",
    "val_loss, val_acc = model.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "print(\"---------- Test Evaluation ----------\")\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 10) 모델 저장\n",
    "#----------------------------------------------------------------------\n",
    "model.save(os.path.join(base_dir, 'efficientnet_lite_finetuned.h5'))\n",
    "print(\"Model saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELA 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELA 변환 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image, ImageChops\n",
    "import io\n",
    "\n",
    "def error_level_analysis_pil(original_img, quality=70, scale=500):\n",
    "    \"\"\"\n",
    "    원본 이미지를 ELA(Error Level Analysis) 후 반환\n",
    "    :param original_img: PIL Image (RGB)\n",
    "    :param quality: JPEG 재압축 품질\n",
    "    :param scale: 차이를 시각적으로 증폭시키기 위한 스케일\n",
    "    :return: ELA 결과를 PIL Image 형태로 반환\n",
    "    \"\"\"\n",
    "    # 메모리에 임시로 JPEG 저장\n",
    "    temp_io = io.BytesIO()\n",
    "    original_img.save(temp_io, 'JPEG', quality=quality)\n",
    "    temp_io.seek(0)\n",
    "\n",
    "    # 재압축된 이미지 불러오기\n",
    "    compressed_img = Image.open(temp_io).convert('RGB')\n",
    "\n",
    "    # 원본과 재압축 이미지를 비교해 차이 계산\n",
    "    diff = ImageChops.difference(original_img, compressed_img)\n",
    "\n",
    "    # 차이를 원하는 배율(scale)만큼 곱해서 증폭\n",
    "    diff = ImageChops.multiply(diff, Image.new('RGB', diff.size, (scale, scale, scale)))\n",
    "    return diff\n",
    "\n",
    "def convert_to_ela(original_dir, ela_dir, quality=70, scale=500, extensions=('.jpg', '.jpeg', '.png')):\n",
    "    \"\"\"\n",
    "    원본 폴더(original_dir) 구조를 따라가며 모든 이미지를 ELA 변환 후,\n",
    "    같은 구조의 ela_dir 폴더에 저장한다.\n",
    "    \n",
    "    :param original_dir: 원본 폴더 (예: Train/Real)\n",
    "    :param ela_dir: ELA 변환된 이미지를 저장할 폴더 (예: ELA_Train/Real)\n",
    "    :param quality: ELA에 사용될 JPEG 품질\n",
    "    :param scale: ELA 차이 증폭 배율\n",
    "    :param extensions: 처리할 이미지 확장자 목록\n",
    "    \"\"\"\n",
    "    if not os.path.exists(ela_dir):\n",
    "        os.makedirs(ela_dir, exist_ok=True)\n",
    "\n",
    "    # 원본 폴더 내 파일/폴더를 순회\n",
    "    for item in os.listdir(original_dir):\n",
    "        src_path = os.path.join(original_dir, item)\n",
    "        dst_path = os.path.join(ela_dir, item)\n",
    "\n",
    "        if os.path.isdir(src_path):\n",
    "            # 서브폴더인 경우, 재귀적으로 진행\n",
    "            convert_to_ela(src_path, dst_path, quality, scale, extensions)\n",
    "        else:\n",
    "            # 파일인 경우\n",
    "            ext = os.path.splitext(item)[1].lower()\n",
    "            if ext in extensions:\n",
    "                # 이미지 파일이면 ELA 적용 후 저장\n",
    "                with Image.open(src_path).convert('RGB') as img:\n",
    "                    ela_img = error_level_analysis_pil(img, quality=quality, scale=scale)\n",
    "                    ela_img.save(dst_path)  # 확장자는 그대로, 혹은 .jpg로 통일 가능\n",
    "            else:\n",
    "                # 그 외 파일(예: txt)은 스킵하거나 복사\n",
    "                pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # base_dir 설정\n",
    "    base_dir = os.getcwd()\n",
    "\n",
    "    # 원본 폴더 구조\n",
    "    train_dir = os.path.join(base_dir, 'Train')\n",
    "    val_dir   = os.path.join(base_dir, 'Validation')\n",
    "    test_dir  = os.path.join(base_dir, 'Test')\n",
    "\n",
    "    # ELA 저장할 폴더 구조\n",
    "    ela_train_dir = os.path.join(base_dir, 'ELA_Train')\n",
    "    ela_val_dir   = os.path.join(base_dir, 'ELA_Validation')\n",
    "    ela_test_dir  = os.path.join(base_dir, 'ELA_Test')\n",
    "\n",
    "    # 각 폴더를 순회하며 ELA 변환\n",
    "    q = 80 # JPEG 품질\n",
    "    s = 800 # ELA 차이 증폭 배율\n",
    "    \n",
    "    convert_to_ela(train_dir, ela_train_dir, quality=q, scale=s)\n",
    "    convert_to_ela(val_dir, ela_val_dir, quality=q, scale=s)\n",
    "    convert_to_ela(test_dir, ela_test_dir, quality=q, scale=s)\n",
    "\n",
    "    print(\"ELA 변환 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6002 images belonging to 2 classes.\n",
      "Found 7324 images belonging to 2 classes.\n",
      "Found 10905 images belonging to 2 classes.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 1280)              3413024   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 1281      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,414,305\n",
      "Trainable params: 1,281\n",
      "Non-trainable params: 3,413,024\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "376/376 [==============================] - 112s 277ms/step - loss: 0.7001 - accuracy: 0.5045 - val_loss: 0.6885 - val_accuracy: 0.5168\n",
      "Epoch 2/10\n",
      "376/376 [==============================] - 18s 48ms/step - loss: 0.6947 - accuracy: 0.5157 - val_loss: 0.6878 - val_accuracy: 0.5531\n",
      "Epoch 3/10\n",
      "376/376 [==============================] - 18s 49ms/step - loss: 0.6955 - accuracy: 0.5038 - val_loss: 0.6871 - val_accuracy: 0.5994\n",
      "Epoch 4/10\n",
      "376/376 [==============================] - 18s 49ms/step - loss: 0.6951 - accuracy: 0.5072 - val_loss: 0.6864 - val_accuracy: 0.5388\n",
      "Epoch 5/10\n",
      "376/376 [==============================] - 18s 49ms/step - loss: 0.6919 - accuracy: 0.5195 - val_loss: 0.6861 - val_accuracy: 0.5225\n",
      "Epoch 6/10\n",
      "376/376 [==============================] - 19s 49ms/step - loss: 0.6916 - accuracy: 0.5272 - val_loss: 0.6855 - val_accuracy: 0.5300\n",
      "Epoch 7/10\n",
      "376/376 [==============================] - 19s 50ms/step - loss: 0.6919 - accuracy: 0.5157 - val_loss: 0.6852 - val_accuracy: 0.5280\n",
      "Epoch 8/10\n",
      "376/376 [==============================] - 19s 50ms/step - loss: 0.6911 - accuracy: 0.5282 - val_loss: 0.6848 - val_accuracy: 0.5248\n",
      "Epoch 9/10\n",
      "376/376 [==============================] - 19s 49ms/step - loss: 0.6884 - accuracy: 0.5363 - val_loss: 0.6845 - val_accuracy: 0.5233\n",
      "Epoch 10/10\n",
      "376/376 [==============================] - 19s 49ms/step - loss: 0.6895 - accuracy: 0.5370 - val_loss: 0.6835 - val_accuracy: 0.5358\n",
      "---------- Validation Evaluation ----------\n",
      "458/458 [==============================] - 4s 9ms/step - loss: 0.6835 - accuracy: 0.5358\n",
      "Validation Loss: 0.6835, Validation Accuracy: 0.5358\n",
      "---------- Test Evaluation ----------\n",
      "682/682 [==============================] - 74s 108ms/step - loss: 0.6946 - accuracy: 0.4921\n",
      "Test Loss: 0.6946, Test Accuracy: 0.4921\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 1) 경로 설정\n",
    "#----------------------------------------------------------------------\n",
    "base_dir = os.getcwd()  # 현재 작업 디렉토리\n",
    "train_dir = os.path.join(base_dir, 'ELA_Train')\n",
    "val_dir = os.path.join(base_dir, 'ELA_Validation')\n",
    "test_dir = os.path.join(base_dir, 'ELA_Test')\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 2) 학습 파라미터 설정\n",
    "#----------------------------------------------------------------------\n",
    "IMAGE_SIZE = (128, 128)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10  # 필요에 따라 조정\n",
    "NUM_CLASSES = 1  # Fake vs Real (이진분류)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 3) ImageDataGenerator 설정\n",
    "#    (데이터 증강 및 전처리)\n",
    "#----------------------------------------------------------------------\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 4) Directory로부터 이미지 로드 (flow_from_directory)\n",
    "#----------------------------------------------------------------------\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=train_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',     # Fake / Real 이진 분류\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    directory=val_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=test_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 5) 사전 학습된 EfficientNet-Lite 모델 불러오기 (TensorFlow Hub 예시)\n",
    "#    - include_top=False 또는 feature-vector 형태 모델을 사용해 상위 레이어 재구성\n",
    "#    - EfficientNet Lite0 ~ Lite4까지 다양한 버전이 있으므로, 아래 URL은 예시입니다.\n",
    "#----------------------------------------------------------------------\n",
    "efficientnet_lite_url = \"https://tfhub.dev/tensorflow/efficientnet/lite0/feature-vector/2\"\n",
    "feature_extractor = hub.KerasLayer(\n",
    "    efficientnet_lite_url,\n",
    "    input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n",
    "    trainable=False  # 혹은 True로 설정하면 파라미터도 업데이트함(Full Fine-tuning)\n",
    ")\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 6) 모델 구성\n",
    "#----------------------------------------------------------------------\n",
    "model = models.Sequential([\n",
    "    feature_extractor,\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation='sigmoid')  # 이진 분류이므로 sigmoid 사용\n",
    "])\n",
    "\n",
    "model.summary()  # 모델 구조 확인\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 7) 모델 컴파일\n",
    "#----------------------------------------------------------------------\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 8) 모델 학습 (Fine-tuning)\n",
    "#----------------------------------------------------------------------\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator\n",
    ")\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 9) 학습 결과 평가\n",
    "#----------------------------------------------------------------------\n",
    "print(\"---------- Validation Evaluation ----------\")\n",
    "val_loss, val_acc = model.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "print(\"---------- Test Evaluation ----------\")\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 10) 모델 저장\n",
    "#----------------------------------------------------------------------\n",
    "model.save(os.path.join(base_dir, 'efficientnet_lite_ela.h5'))\n",
    "print(\"Model saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6002 images belonging to 2 classes.\n",
      "Found 39428 images belonging to 2 classes.\n",
      "Found 10905 images belonging to 2 classes.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 16)        448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 32, 32, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 16, 16, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 8, 8, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 8, 8, 32)          9248      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 4, 4, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                16416     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,033\n",
      "Trainable params: 40,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub  # (사용하지 않아도 무방하나, 편의상 import)\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 1) 경로 설정\n",
    "#----------------------------------------------------------------------\n",
    "base_dir = os.getcwd()  # 현재 작업 디렉토리\n",
    "train_dir = os.path.join(base_dir, 'Train')\n",
    "val_dir   = os.path.join(base_dir, 'Validation')\n",
    "test_dir  = os.path.join(base_dir, 'Test')\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 2) 학습 파라미터 설정\n",
    "#----------------------------------------------------------------------\n",
    "IMAGE_SIZE = (64, 64)   # 이미지를 256x256으로 리사이즈\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10               # 필요에 따라 조정\n",
    "NUM_CLASSES = 1           # Fake vs Real (이진분류)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 3) ImageDataGenerator 설정 (데이터 증강 및 전처리)\n",
    "#----------------------------------------------------------------------\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 4) Directory로부터 이미지 로드 (flow_from_directory)\n",
    "#----------------------------------------------------------------------\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=train_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',  # Fake / Real 이진 분류\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    directory=val_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=test_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 5) 모델 정의 (간단한 CNN)\n",
    "#    - 입력: 256x256 RGB\n",
    "#    - 출력: 이진 분류(sigmoid)\n",
    "#----------------------------------------------------------------------\n",
    "model = models.Sequential()\n",
    "\n",
    "# 첫 번째 Conv2D 레이어\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu', \n",
    "                        padding='same', \n",
    "                        input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 두 번째 Conv2D 레이어\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 세 번째 Conv2D 레이어\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 네 번째 Conv2D 레이어\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 특징 맵을 1차원으로 펼쳐서 Dense에 전달\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# 완전 연결 레이어(Dense)\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "\n",
    "# 출력 레이어: 이진 분류 → 1개의 노드 + sigmoid\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 6) 모델 컴파일\n",
    "#----------------------------------------------------------------------\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "376/376 [==============================] - 50s 126ms/step - loss: 0.6756 - accuracy: 0.5956 - val_loss: 0.6859 - val_accuracy: 0.5657\n",
      "Epoch 2/10\n",
      "376/376 [==============================] - 29s 77ms/step - loss: 0.6193 - accuracy: 0.6753 - val_loss: 0.7460 - val_accuracy: 0.5801\n",
      "Epoch 3/10\n",
      "376/376 [==============================] - 28s 74ms/step - loss: 0.5771 - accuracy: 0.7079 - val_loss: 0.7086 - val_accuracy: 0.6080\n",
      "Epoch 4/10\n",
      "376/376 [==============================] - 28s 74ms/step - loss: 0.5508 - accuracy: 0.7271 - val_loss: 0.6434 - val_accuracy: 0.6457\n",
      "Epoch 5/10\n",
      "376/376 [==============================] - 28s 74ms/step - loss: 0.5397 - accuracy: 0.7358 - val_loss: 0.6685 - val_accuracy: 0.6285\n",
      "Epoch 6/10\n",
      "376/376 [==============================] - 28s 76ms/step - loss: 0.5286 - accuracy: 0.7426 - val_loss: 0.6704 - val_accuracy: 0.6409\n",
      "Epoch 7/10\n",
      "376/376 [==============================] - 28s 74ms/step - loss: 0.5164 - accuracy: 0.7532 - val_loss: 0.6800 - val_accuracy: 0.6348\n",
      "Epoch 8/10\n",
      "376/376 [==============================] - 28s 76ms/step - loss: 0.5066 - accuracy: 0.7667 - val_loss: 0.7346 - val_accuracy: 0.6204\n",
      "Epoch 9/10\n",
      "376/376 [==============================] - 28s 76ms/step - loss: 0.5028 - accuracy: 0.7609 - val_loss: 0.7394 - val_accuracy: 0.5928\n",
      "Epoch 10/10\n",
      "376/376 [==============================] - 28s 75ms/step - loss: 0.4916 - accuracy: 0.7772 - val_loss: 0.7778 - val_accuracy: 0.6168\n",
      "---------- Validation Evaluation ----------\n",
      "2465/2465 [==============================] - 22s 9ms/step - loss: 0.7778 - accuracy: 0.6168\n",
      "Validation Loss: 0.7778, Validation Accuracy: 0.6168\n",
      "---------- Test Evaluation ----------\n",
      "682/682 [==============================] - 79s 116ms/step - loss: 0.7770 - accuracy: 0.6083\n",
      "Test Loss: 0.7770, Test Accuracy: 0.6083\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 7) 모델 학습\n",
    "#----------------------------------------------------------------------\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator\n",
    ")\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 8) 학습 결과 평가\n",
    "#----------------------------------------------------------------------\n",
    "print(\"---------- Validation Evaluation ----------\")\n",
    "val_loss, val_acc = model.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "print(\"---------- Test Evaluation ----------\")\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 9) 모델 저장\n",
    "#----------------------------------------------------------------------\n",
    "model.save(os.path.join(base_dir, 'custom_cnn_finetuned.h5'))\n",
    "print(\"Model saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
